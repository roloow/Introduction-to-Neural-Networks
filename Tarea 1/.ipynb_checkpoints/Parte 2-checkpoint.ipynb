{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código y trabajo\n",
    "\n",
    "Parte 2\n",
    "-----\n",
    "\n",
    "\n",
    "**Restricción:** `No usar librerías especializadas, excepto numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado\n",
    "* Escriba un programa que permita entrenar una red FF con 1 capa escondida (H neuronas) y O neuronas de salida, **sin usar librerías**, excepto eventualmente *numpy* para implementar operaciones básicas de álgebra lineal. Por simplicidad, asuma que todas las neuronas implementan una función de activación diferenciable y que la función de error (*loss function*) también lo es. Especiﬁque explícitamente las funciones anteriores, así como sus gradientes. Escriba funciones para: \n",
    "    1. Dar valores iniciales a los pesos de la red\n",
    "    2. Implementar el forward pass\n",
    "    3. Implementar el backward pass\n",
    "    4. Implementar la rutina principal de entrenamiento, adoptando, por simplicidad, la variante cíclica de SGD (un ejemplo a la vez, pero iterando cíclicamente sobre el conjunto de entrenamiento) con una tasa de aprendizaje y número de ciclos ﬁjos (epochs).\n",
    "----------\n",
    "*  Escriba una función que permita hacer predicciones mediante una red FF con 1 capa escondida (H neuronas) y O neuronas de salida, sin usar librerías, excepto eventualmente numpy. Escriba una función vectorizada que implemente el **forward pass** sobre un conjunto de *n<sub>test</sub>* ejemplos.\n",
    "\n",
    "----------\n",
    "\n",
    "* Demuestre que sus programas funcionan en un problema de clasiﬁcación, eligiendo funciones de error y de activación apropiadas. Para esto utilice el dataset seeds, disponible en [UCI](http://archive.ics.uci.edu/ml/) y correspondiente a la clasiﬁcación de distintos tipos de semillas (3 clases) (recuerde que usualmente es conveniente normalizar los datos antes de trabajar con el modelo).\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt'\n",
    "df = pd.read_csv(url, sep=r'\\s+',header=None)\n",
    "X_train = df.ix[:,0:6]\n",
    "y_train = df.ix[:,7]\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "```\n",
    "Para evaluar los resultados, construya un gráﬁco correspondiente al error de clasiﬁcación versus número de *epochs*, utilizando sólo el conjunto de entrenamiento (el objetivo de esta sección es familiarizarse con el algoritmo BP, no encontrar la mejor red). Graﬁque también la evolución de la función objetivo utilizada para el entrenamiento.\n",
    "\n",
    "----------\n",
    "\n",
    "* Construya, sin usar librerías, excepto eventualmente numpy para implementar operaciones básicas de álgebra lineal, una variante de su programa anterior que entrene la red utilizando *weight-decay*.\n",
    "\n",
    "### Solución\n",
    "\n",
    "Utilizando la [siguiente referencia](http://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/) implementaremos una red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**A) Programa de entrenamiento**\n",
    "\n",
    "Esta sección ha sido subdividida en cuatro etapas, las cuales involucran desde la inicialización de la red hasta la implementación de la rutina principal de entrenamiento. Para este trabajo deberemos primero crear la nueva red, donde cada neurona tiene un conjunto de pesos, durante el entrenamiento tendremos que almacenar propiedades adicionales es por este motivo que definiremos a una **neurona** como un *diccionario* y guardaremos los pesos con el nombre *weights*. Por otra parte, una red se organiza en *capas*, donde la capa de entrada (*input layer*) es una fila del *dataset de entrenamiento*. La verdadera primera capa es la capa escondida (*hidden layer*), la cual es seguida por la capa de salida (*output layer*) que contiene una neurona por cada valor de una clase. Como podemos inferir, organizaremos una capa como un arreglo (*lista*) de neuronas (*diccionarios*) y una red como un arreglo (*lista*) de capas.\n",
    "\n",
    "```python\n",
    "neuron = dict()\n",
    "neuron['weights'] = [value1, value2, value3, ..., valueN]\n",
    "\n",
    "layer = list()\n",
    "...\n",
    "layer = [neuron1, neuron2, neuron3, ..., neuronM]\n",
    "\n",
    "network = list()\n",
    "...\n",
    "network = [layer1, layer2, layer3, ..., layerZ]\n",
    "```\n",
    "\n",
    "Para dar valores iniciales a la red de pesos utilizaremos números pequeños aleatoreos (**Libreria random**), los cuales estarán en el rango de 0 a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#            initialize_network\n",
    "#--------------------------------------------\n",
    "#   FUNCTION_IN_PARAMETERS_DEFINITION\n",
    "#   n_inputs:  integer number of inputs\n",
    "#   n_hidden:  integer number of neurons to \n",
    "#              have in the hidden layer\n",
    "#   n_outputs: integer number of outputs\n",
    "#\n",
    "#   FUNCTION_OUT_PARAMETERS_DEFINITION\n",
    "#   out:    list of list of dictionaries that\n",
    "#           means an array of layers of neurons\n",
    "#   FUNCTION_CODING\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = list()\n",
    "    for i in range(n_hidden):\n",
    "        hidden_layer.append({'weights': [rd.random() for j in range(n_inputs + 1)]})\n",
    "    network.append(hidden_layer)\n",
    "    \n",
    "    output_layer = list()\n",
    "    for i in range(n_outputs):\n",
    "        output_layer.append({'weights': [rd.random() for j in range(n_hidden + 1)]})\n",
    "    network.append(output_layer)\n",
    "    \n",
    "    return network\n",
    "#   FUNCTION_EXPLANATION\n",
    "#   Creates a new neural network ready for \n",
    "#   training. It accepts three parameters,\n",
    "#   the number of inputs, the number of neurons\n",
    "#   to have in the hidden layer and the number \n",
    "#   of outputs.\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que la capa escondida contiene *n<sub>hidden</sub>* neuronas y cada neurona tiene *n<sub>inputs</sub> + 1* pesos, una por cada columna en el *dataset* y uno adicional para el **bias** (*sesgo*). También se puede notar que la capa de salida que se conecta a la capa escondida presenta *n<sub>outputs</sub>* neuronas y cada una con *n<sub>hidden</sub> + 1* pesos. Esto permite que cada neurona en la capa de salida esté conectada (*tenga un peso*) con una neurona en la capa escondida.\n",
    "\n",
    "#### Testing de la función\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n",
      "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n"
     ]
    }
   ],
   "source": [
    "# Mantener la aleatoriedad controlada\n",
    "rd.seed(1)\n",
    "\n",
    "# Testing\n",
    "network = initialize_network(2, 1, 2)\n",
    "for layer in network:\n",
    "    print layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede notar que la capa escondida tiene una neurona con 2 pesos además del sesgo y la capa de salida tiene 2 neuronas, cada una con 1 peso además del sesgo. Ahora que ya sabemos como crear e inicializar una red, veamos como calcular la salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "#### Forward Propagation\n",
    "\n",
    "Se puede calcular la salida de una red neuronal mediante la propagación de una señal de entrada a través de las capas hasta que la capa de salida retorne el valor.\n",
    "\n",
    "Para este proceso tenemos tres partes:\n",
    "1. Activación de la neurona\n",
    "2. Transferencia de la neurona\n",
    "3. Propagación hacia adelante\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es calcular la activación de una neurona dadas una entrada. Para este caso, podremos decir que la entrada es una fila del *DataSet* de entrenamiento al igual que en el caso de la capa escondida. La función para calcular es muy cercana a una regresión lineal, donde se calcula la suma de los pesos para cada entrada. El sesgo puede considerarse como aparte, o como que es multiplicado siempre por 1.\n",
    "\n",
    "\\begin{equation*}\n",
    "activation = ( \\sum_{k=1}^n weight_i * input_i ) + bias\n",
    "\\end{equation*}\n",
    "\n",
    "Gracias a nuestra definición previa podremos siempre definir al *bias* como el último valor del arreglo de pesos, entonces una implementación de esta función es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#            activate_neuron\n",
    "#--------------------------------------------\n",
    "#   FUNCTION_IN_PARAMETERS_DEFINITION\n",
    "#   weights:  list of integers\n",
    "#   inputs:   list of inputs\n",
    "#\n",
    "#   FUNCTION_OUT_PARAMETERS_DEFINITION\n",
    "#   out:    integer value that represent the \n",
    "#           activation equation\n",
    "#   FUNCTION_CODING\n",
    "def activate_neuron(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    quantum = len(weights) - 1\n",
    "    for i in range(quantum):\n",
    "        activation += weights[i] * inputs[i]\n",
    "        \n",
    "    return activation\n",
    "#   FUNCTION_EXPLANATION\n",
    "#   Calculate the neuron activation for an input\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "Ahora que tenemos nuestra función de activación, veamos como utilizarla. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
