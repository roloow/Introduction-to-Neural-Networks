{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Código y trabajo\n",
    "\n",
    "Parte 2\n",
    "-----\n",
    "\n",
    "\n",
    "**Restricción:** `No usar librerías especializadas, excepto numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Enunciado\n",
    "* Escriba un programa que permita entrenar una red FF con 1 capa escondida (H neuronas) y O neuronas de salida, **sin usar librerías**, excepto eventualmente *numpy* para implementar operaciones básicas de álgebra lineal. Por simplicidad, asuma que todas las neuronas implementan una función de activación diferenciable y que la función de error (*loss function*) también lo es. Especiﬁque explícitamente las funciones anteriores, así como sus gradientes. Escriba funciones para: \n",
    "    1. Dar valores iniciales a los pesos de la red\n",
    "    2. Implementar el forward pass\n",
    "    3. Implementar el backward pass\n",
    "    4. Implementar la rutina principal de entrenamiento, adoptando, por simplicidad, la variante cíclica de SGD (un ejemplo a la vez, pero iterando cíclicamente sobre el conjunto de entrenamiento) con una tasa de aprendizaje y número de ciclos ﬁjos (epochs).\n",
    "----------\n",
    "*  Escriba una función que permita hacer predicciones mediante una red FF con 1 capa escondida (H neuronas) y O neuronas de salida, sin usar librerías, excepto eventualmente numpy. Escriba una función vectorizada que implemente el **forward pass** sobre un conjunto de *n<sub>test</sub>* ejemplos.\n",
    "\n",
    "----------\n",
    "\n",
    "* Demuestre que sus programas funcionan en un problema de clasiﬁcación, eligiendo funciones de error y de activación apropiadas. Para esto utilice el dataset seeds, disponible en [UCI](http://archive.ics.uci.edu/ml/) y correspondiente a la clasiﬁcación de distintos tipos de semillas (3 clases) (recuerde que usualmente es conveniente normalizar los datos antes de trabajar con el modelo).\n",
    "\n",
    "```python\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import pandas as pd\n",
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt'\n",
    "df = pd.read_csv(url, sep=r'\\s+',header=None)\n",
    "X_train = df.ix[:,0:6]\n",
    "y_train = df.ix[:,7]\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)\n",
    "\n",
    "```\n",
    "Para evaluar los resultados, construya un gráﬁco correspondiente al error de clasiﬁcación versus número de *epochs*, utilizando sólo el conjunto de entrenamiento (el objetivo de esta sección es familiarizarse con el algoritmo BP, no encontrar la mejor red). Graﬁque también la evolución de la función objetivo utilizada para el entrenamiento.\n",
    "\n",
    "----------\n",
    "\n",
    "* Construya, sin usar librerías, excepto eventualmente numpy para implementar operaciones básicas de álgebra lineal, una variante de su programa anterior que entrene la red utilizando *weight-decay*.\n",
    "\n",
    "### Solución\n",
    "\n",
    "Utilizando la [siguiente referencia](http://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/) implementaremos una red."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "**A) Programa de entrenamiento**\n",
    "\n",
    "Esta sección ha sido subdividida en cuatro etapas, las cuales involucran desde la inicialización de la red hasta la implementación de la rutina principal de entrenamiento. Para este trabajo deberemos primero crear la nueva red, donde cada neurona tiene un conjunto de pesos, durante el entrenamiento tendremos que almacenar propiedades adicionales es por este motivo que definiremos a una **neurona** como un *diccionario* y guardaremos los pesos con el nombre *weights*. Por otra parte, una red se organiza en *capas*, donde la capa de entrada (*input layer*) es una fila del *dataset de entrenamiento*. La verdadera primera capa es la capa escondida (*hidden layer*), la cual es seguida por la capa de salida (*output layer*) que contiene una neurona por cada valor de una clase. Como podemos inferir, organizaremos una capa como un arreglo (*lista*) de neuronas (*diccionarios*) y una red como un arreglo (*lista*) de capas.\n",
    "\n",
    "```python\n",
    "neuron = dict()\n",
    "neuron['weights'] = [value1, value2, value3, ..., valueN]\n",
    "\n",
    "layer = list()\n",
    "...\n",
    "layer = [neuron1, neuron2, neuron3, ..., neuronM]\n",
    "\n",
    "network = list()\n",
    "...\n",
    "network = [layer1, layer2, layer3, ..., layerZ]\n",
    "```\n",
    "\n",
    "Para dar valores iniciales a la red de pesos utilizaremos números pequeños aleatoreos (**Libreria random**), los cuales estarán en el rango de 0 a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#            initialize_network\n",
    "#--------------------------------------------\n",
    "#   FUNCTION_IN_PARAMETERS_DEFINITION\n",
    "#   n_inputs:  integer number of inputs\n",
    "#   n_hidden:  integer number of neurons to \n",
    "#              have in the hidden layer\n",
    "#   n_outputs: integer number of outputs\n",
    "#\n",
    "#   FUNCTION_OUT_PARAMETERS_DEFINITION\n",
    "#   out:    list of list of dictionaries that\n",
    "#           means an array of layers of neurons\n",
    "#   FUNCTION_CODING\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = list()\n",
    "    for i in range(n_hidden):\n",
    "        hidden_layer.append({'weights': [rd.random() for j in range(n_inputs + 1)]})\n",
    "    network.append(hidden_layer)\n",
    "    \n",
    "    output_layer = list()\n",
    "    for i in range(n_outputs):\n",
    "        output_layer.append({'weights': [rd.random() for j in range(n_hidden + 1)]})\n",
    "    network.append(output_layer)\n",
    "    \n",
    "    return network\n",
    "#   FUNCTION_EXPLANATION\n",
    "#   Creates a new neural network ready for \n",
    "#   training. It accepts three parameters,\n",
    "#   the number of inputs, the number of neurons\n",
    "#   to have in the hidden layer and the number \n",
    "#   of outputs.\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que la capa escondida contiene *n<sub>hidden</sub>* neuronas y cada neurona tiene *n<sub>inputs</sub> + 1* pesos, una por cada columna en el *dataset* y uno adicional para el **bias** (*sesgo*). También se puede notar que la capa de salida que se conecta a la capa escondida presenta *n<sub>outputs</sub>* neuronas y cada una con *n<sub>hidden</sub> + 1* pesos. Esto permite que cada neurona en la capa de salida esté conectada (*tenga un peso*) con una neurona en la capa escondida.\n",
    "\n",
    "#### Testing de la función\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'weights': [0.13436424411240122, 0.8474337369372327, 0.763774618976614]}]\n",
      "[{'weights': [0.2550690257394217, 0.49543508709194095]}, {'weights': [0.4494910647887381, 0.651592972722763]}]\n"
     ]
    }
   ],
   "source": [
    "# Mantener la aleatoriedad controlada\n",
    "rd.seed(1)\n",
    "\n",
    "# Testing\n",
    "network = initialize_network(2, 1, 2)\n",
    "for layer in network:\n",
    "    print layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Se puede notar que la capa escondida tiene una neurona con 2 pesos además del sesgo y la capa de salida tiene 2 neuronas, cada una con 1 peso además del sesgo. Ahora que ya sabemos como crear e inicializar una red, veamos como calcular la salida."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "#### Forward Propagate\n",
    "\n",
    "Se puede calcular la salida de una red neuronal mediante la propagación de una señal de entrada a través de las capas hasta que la capa de salida retorne el valor.\n",
    "\n",
    "Para este proceso tenemos tres partes:\n",
    "1. Activación de la neurona\n",
    "2. Transferencia de la neurona\n",
    "3. Propagación hacia adelante\n",
    "\n",
    "----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer paso es calcular la activación de una neurona dadas una entrada. Para este caso, podremos decir que la entrada es una fila del *DataSet* de entrenamiento al igual que en el caso de la capa escondida. La función para calcular es muy cercana a una regresión lineal, donde se calcula la suma de los pesos para cada entrada. El sesgo puede considerarse como aparte, o como que es multiplicado siempre por 1.\n",
    "\n",
    "\\begin{equation*}\n",
    "activation = ( \\sum_{k=1}^n weight_i * input_i ) + bias\n",
    "\\end{equation*}\n",
    "\n",
    "Gracias a nuestra definición previa podremos siempre definir al *bias* como el último valor del arreglo de pesos, entonces una implementación de esta función es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#            activate_neuron\n",
    "#--------------------------------------------\n",
    "#   FUNCTION_IN_PARAMETERS_DEFINITION\n",
    "#   weights:  list of floats\n",
    "#   inputs:   list of floats\n",
    "#\n",
    "#   FUNCTION_OUT_PARAMETERS_DEFINITION\n",
    "#   out:    integer value that represent the \n",
    "#           activation equation\n",
    "#   FUNCTION_CODING\n",
    "def activate_neuron(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    quantum = len(weights) - 1\n",
    "    for i in range(quantum):\n",
    "        activation += weights[i] * inputs[i]\n",
    "        \n",
    "    return activation\n",
    "#   FUNCTION_EXPLANATION\n",
    "#   Calculate the neuron activation for an input\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "Ahora que tenemos nuestra función de activación, veamos como utilizarla. Para poder lograr obtener el resultado deberemos utilizar la función de activación y transferir este valor a través de las capas.\n",
    "\n",
    "Existen diferentes funciones de transferencia pero tradicionalmente se utiliza la [función Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function). Otras opciones pueden ser:\n",
    "\n",
    "- [Tangente hiperbólica](https://en.wikipedia.org/wiki/Hyperbolic_function)\n",
    "- [Rectificante](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\n",
    "La activación de sigmoid, también conocida como función logística, pede tomar cualquier valor y producir un número entre 0 y 1 dentro de su curva, la cual tiene una forma parecida a una S aplastada.\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\" width=\"400\" height=\"800\" />\n",
    "\n",
    "Una particularidad de esta función es que se le puede calcular facilmente la derivada, lo cual facilitará implementaciones posteriores. Entonces, definimos la función sigmoid como\n",
    "\n",
    "\\begin{equation*}\n",
    "S(t) = \\frac{1}{1 + e^{-t}}\n",
    "\\end{equation*}\n",
    "\n",
    "Lo cual implementaremos de la siguiente forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#            transfer\n",
    "#--------------------------------------------\n",
    "#   FUNCTION_IN_PARAMETERS_DEFINITION\n",
    "#   value: float number\n",
    "#\n",
    "#   FUNCTION_OUT_PARAMETERS_DEFINITION\n",
    "#   out:    float number of transferation\n",
    "#   FUNCTION_CODING\n",
    "def transfer(value):\n",
    "    transfer_value = 1.0 / (1 + np.exp(-value))\n",
    "    \n",
    "    return transfer_value\n",
    "#   FUNCTION_EXPLANATION\n",
    "#   Calculate the sigmoid function value of input\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "Ya habiendo definido nuestra base, la propagación es una aplicación directa. Trabajando sobre cada capa de la red se calcula la salida de cada neurona, la cual sera utilizada como input para la siguiente capa.\n",
    "\n",
    "Debido a la estructura que elegimos para la neurona, podremos guardar el valor de salida como una llave del diccionario, además dentro de la iteración de capas deberemos almacenar todos estos valores para que al cambio de capa sean utilizados como los nuevos valores de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#            forward_propagate\n",
    "#--------------------------------------------\n",
    "#   FUNCTION_IN_PARAMETERS_DEFINITION\n",
    "#   network: list of lists\n",
    "#   row:     list of float values\n",
    "#\n",
    "#   FUNCTION_OUT_PARAMETERS_DEFINITION\n",
    "#   out:    list of float values\n",
    "#   FUNCTION_CODING\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = list()\n",
    "        for neuron in layer:\n",
    "            activation_value = activate_neuron(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation_value)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "        \n",
    "    return inputs\n",
    "#   FUNCTION_EXPLANATION\n",
    "#   Propagates the values from a row of values\n",
    "#   till the output's layer and returns the last\n",
    "#   list of values\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "-----------------\n",
    "Finalmente debemos realizar un testing del funcionamiento. Utilizando los valores previos de red y generando una lista a propagar de valores 1 y 0 obtendremos una salida de dos valores dado que la red que habiamos configurado contenia dos neuronal en su capa final."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.66299701298528868, 0.72531607252797481]\n"
     ]
    }
   ],
   "source": [
    "row = [1, 0, None]\n",
    "output = forward_propagate(network, row)\n",
    "\n",
    "print output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward propagate\n",
    "\n",
    "Esta sección de trabajo es la que permite determinar errores comparando el valor esperado con el valor propagado de la red. Este error es revisado a través de la red desde la capa final hasta la capa escondida asignando responsabilidad sobre el error y actualizando los pesos a medida que avanza.\n",
    "\n",
    "La propagación del error tiene una base científica en el *Cálculo*, sin embargo como simplificación nos mantendremos al margen de su funcionamiento y nos concentraremos en su uso.\n",
    "\n",
    "Al igual que la sección anterior, podemos dividir el trabajo en dos partes\n",
    "\n",
    "1. Derivada de transferencia\n",
    "2. Error de propagación hacia atras\n",
    "-----------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lo primero es calcular la derivada de transferencia que para este informe estamos utilizando la función **Sigmoid**, cuya derivada se expresa de la siguiente forma\n",
    "\n",
    "\\begin{equation*}\n",
    "S'(t) = S(t)(1 - S(t))\n",
    "\\end{equation*}\n",
    "\n",
    "Esto nos permite implementarla de una manera muy sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#            transfer_derivative\n",
    "#--------------------------------------------\n",
    "#   FUNCTION_IN_PARAMETERS_DEFINITION\n",
    "#   S:   float number representing S(t)\n",
    "#\n",
    "#   FUNCTION_OUT_PARAMETERS_DEFINITION\n",
    "#   out:    float number \n",
    "#   FUNCTION_CODING\n",
    "def transfer_derivative(S):\n",
    "    derivative = S * (1.0 - S)\n",
    "    \n",
    "    return derivative\n",
    "#   FUNCTION_EXPLANATION\n",
    "#   Calculate the transfer derivative \n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "Ahora que tenemos la función implementada, debemos calcular el error para cada salida de la neurona, esto nos dará la señal de error que propagaremos hacia atrás a través de la red.\n",
    "\n",
    "Para obtener el error que mencionamos recién es necesario realizar una comparación entre el valor esperado y el valor resultante, a lo cual además multiplicaremos la derivada de la transferencia, esto se expresa matemáticamente como\n",
    "\n",
    "\\begin{equation*}\n",
    "Error = (E(x) - O(x)) * \\frac{\\partial Sigmoid(x)}{\\partial x}\n",
    "\\end{equation*}\n",
    "\n",
    "Este cálculo de error es usado para neuronas en la capa de salida, donde el valor esperado es el valor de la clase. La señal para una neurona en la capa escondida es calculada como el error de los pesos de cada neurona en la capa de salida.\n",
    "\n",
    "Ahora bien, la señal del error propagado se acumula y usa para determinar el error de la neurona en la capa escondida de la siguiente forma\n",
    "\n",
    "\\begin{equation*}\n",
    "Error = (weight_i * error_j) *\n",
    "\\end{equation*}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
