{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<center><h1>Introducción a Redes Neuronales Artificiales</h1></center>\n",
    "<center><h2>Tarea 1 - Perceptrones Multicapa o Redes Feed-Forward</h2></center>\n",
    "\n",
    "\n",
    "|Integrante|Rol|Correo|\n",
    "|:-|:-|:-|\n",
    "|Ricardo Carrasco|201204510-k|<ricardo.carrasco.12@sansano.usm.cl>|\n",
    "|Rolando Casanueva|201204505-3|<rolando.casanueva.12@sansano.usm.cl>|\n",
    "\n",
    "\n",
    "## Desarrollo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "### Parte 2\n",
    "**Restricción:** `No usar librerías especializadas, excepto numpy`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import random as rd\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Utilizando la [siguiente referencia](http://machinelearningmastery.com/implement-backpropagation-algorithm-scratch-python/) implementaremos una red.\n",
    "\n",
    "**A) Programa de entrenamiento**\n",
    "\n",
    "Esta sección ha sido subdividida en cuatro etapas, las cuales involucran desde la inicialización de la red hasta la implementación de la rutina principal de entrenamiento. Para este trabajo deberemos primero crear la nueva red, donde cada neurona tiene un conjunto de pesos, durante el entrenamiento tendremos que almacenar propiedades adicionales es por este motivo que definiremos a una **neurona** como un *diccionario* y guardaremos los pesos con el nombre *weights*. Por otra parte, una red se organiza en *capas*, donde la capa de entrada (*input layer*) es una fila del *dataset de entrenamiento*. La verdadera primera capa es la capa escondida (*hidden layer*), la cual es seguida por la capa de salida (*output layer*) que contiene una neurona por cada valor de una clase. Como podemos inferir, organizaremos una capa como un arreglo (*lista*) de neuronas (*diccionarios*) y una red como un arreglo (*lista*) de capas.\n",
    "\n",
    "```python\n",
    "neuron = dict()\n",
    "neuron['weights'] = [value1, value2, value3, ..., valueN]\n",
    "\n",
    "layer = list()\n",
    "...\n",
    "layer = [neuron1, neuron2, neuron3, ..., neuronM]\n",
    "\n",
    "network = list()\n",
    "...\n",
    "network = [layer1, layer2, layer3, ..., layerZ]\n",
    "```\n",
    "\n",
    "Para dar valores iniciales a la red de pesos utilizaremos números pequeños aleatoreos (**Libreria random**), los cuales estarán en el rango de 0 a 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#            initialize_network\n",
    "#--------------------------------------------\n",
    "#   FUNCTION_IN_PARAMETERS_DEFINITION\n",
    "#   n_inputs:  integer number of inputs\n",
    "#   n_hidden:  integer number of neurons to \n",
    "#              have in the hidden layer\n",
    "#   n_outputs: integer number of outputs\n",
    "#\n",
    "#   FUNCTION_OUT_PARAMETERS_DEFINITION\n",
    "#   out:    list of list of dictionaries that\n",
    "#           means an array of layers of neurons\n",
    "#   FUNCTION_CODING\n",
    "def initialize_network(n_inputs, n_hidden, n_outputs):\n",
    "    network = list()\n",
    "    hidden_layer = list()\n",
    "    for i in range(n_hidden):\n",
    "        hidden_layer.append({'weights': [rd.random() for j in range(n_inputs + 1)]})\n",
    "    network.append(hidden_layer)\n",
    "    \n",
    "    output_layer = list()\n",
    "    for i in range(n_outputs):\n",
    "        output_layer.append({'weights': [rd.random() for j in range(n_hidden + 1)]})\n",
    "    network.append(output_layer)\n",
    "    \n",
    "    return network\n",
    "#   FUNCTION_EXPLANATION\n",
    "#   Creates a new neural network ready for \n",
    "#   training. It accepts three parameters,\n",
    "#   the number of inputs, the number of neurons\n",
    "#   to have in the hidden layer and the number \n",
    "#   of outputs.\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos notar que la capa escondida contiene *n<sub>hidden</sub>* neuronas y cada neurona tiene *n<sub>inputs</sub> + 1* pesos, una por cada columna en el *dataset* y uno adicional para el **bias** (*sesgo*). También se puede notar que la capa de salida que se conecta a la capa escondida presenta *n<sub>outputs</sub>* neuronas y cada una con *n<sub>hidden</sub> + 1* pesos. Esto permite que cada neurona en la capa de salida esté conectada (*tenga un peso*) con una neurona en la capa escondida.\n",
    "\n",
    "----------------\n",
    "#### Forward Propagate\n",
    "\n",
    "Se puede calcular la salida de una red neuronal mediante la propagación de una señal de entrada a través de las capas hasta que la capa de salida retorne el valor.\n",
    "\n",
    "Para este proceso tenemos tres partes:\n",
    "1. Activación de la neurona\n",
    "2. Transferencia de la neurona\n",
    "3. Propagación hacia adelante\n",
    "\n",
    "----------------\n",
    "El primer paso es calcular la activación de una neurona dadas una entrada. Para este caso, podremos decir que la entrada es una fila del *DataSet* de entrenamiento al igual que en el caso de la capa escondida. La función para calcular es muy cercana a una regresión lineal, donde se calcula la suma de los pesos para cada entrada. El sesgo puede considerarse como aparte, o como que es multiplicado siempre por 1.\n",
    "\n",
    "\\begin{equation*}\n",
    "activation = ( \\sum_{k=1}^n weight_i * input_i ) + bias\n",
    "\\end{equation*}\n",
    "\n",
    "Gracias a nuestra definición previa podremos siempre definir al *bias* como el último valor del arreglo de pesos, entonces una implementación de esta función es"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#            activate_neuron\n",
    "#--------------------------------------------\n",
    "#   FUNCTION_IN_PARAMETERS_DEFINITION\n",
    "#   weights:  list of floats\n",
    "#   inputs:   list of floats\n",
    "#\n",
    "#   FUNCTION_OUT_PARAMETERS_DEFINITION\n",
    "#   out:    integer value that represent the \n",
    "#           activation equation\n",
    "#   FUNCTION_CODING\n",
    "def activate_neuron(weights, inputs):\n",
    "    activation = weights[-1]\n",
    "    quantum = len(weights) - 1\n",
    "    for i in range(quantum):\n",
    "        activation += weights[i] * inputs[i]\n",
    "        \n",
    "    return activation\n",
    "#   FUNCTION_EXPLANATION\n",
    "#   Calculate the neuron activation for an input\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "Ahora que tenemos nuestra función de activación, veamos como utilizarla. Para poder lograr obtener el resultado deberemos utilizar la función de activación y transferir este valor a través de las capas.\n",
    "\n",
    "Existen diferentes funciones de transferencia pero tradicionalmente se utiliza la [función Sigmoid](https://en.wikipedia.org/wiki/Sigmoid_function). Otras opciones pueden ser:\n",
    "\n",
    "- [Tangente hiperbólica](https://en.wikipedia.org/wiki/Hyperbolic_function)\n",
    "- [Rectificante](https://en.wikipedia.org/wiki/Rectifier_(neural_networks)\n",
    "\n",
    "La activación de sigmoid, también conocida como función logística, pede tomar cualquier valor y producir un número entre 0 y 1 dentro de su curva, la cual tiene una forma parecida a una S aplastada.\n",
    "<img src=\"https://upload.wikimedia.org/wikipedia/commons/8/88/Logistic-curve.svg\" width=\"400\" height=\"800\" />\n",
    "\n",
    "Una particularidad de esta función es que se le puede calcular facilmente la derivada, lo cual facilitará implementaciones posteriores. Entonces, definimos la función sigmoid como\n",
    "\n",
    "\\begin{equation*}\n",
    "S(t) = \\frac{1}{1 + e^{-t}}\n",
    "\\end{equation*}\n",
    "\n",
    "Lo cual implementaremos de la siguiente forma"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#            transfer\n",
    "#--------------------------------------------\n",
    "#   FUNCTION_IN_PARAMETERS_DEFINITION\n",
    "#   value: float number\n",
    "#\n",
    "#   FUNCTION_OUT_PARAMETERS_DEFINITION\n",
    "#   out:    float number of transferation\n",
    "#   FUNCTION_CODING\n",
    "def transfer(value):\n",
    "    transfer_value = 1.0 / (1 + np.exp(-value))\n",
    "    \n",
    "    return transfer_value\n",
    "#   FUNCTION_EXPLANATION\n",
    "#   Calculate the sigmoid function value of input\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "----------------\n",
    "Ya habiendo definido nuestra base, la propagación es una aplicación directa. Trabajando sobre cada capa de la red se calcula la salida de cada neurona, la cual sera utilizada como input para la siguiente capa.\n",
    "\n",
    "Debido a la estructura que elegimos para la neurona, podremos guardar el valor de salida como una llave del diccionario, además dentro de la iteración de capas deberemos almacenar todos estos valores para que al cambio de capa sean utilizados como los nuevos valores de entrada."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#            forward_propagate\n",
    "#--------------------------------------------\n",
    "#   FUNCTION_IN_PARAMETERS_DEFINITION\n",
    "#   network: list of lists\n",
    "#   row:     list of float values\n",
    "#\n",
    "#   FUNCTION_OUT_PARAMETERS_DEFINITION\n",
    "#   out:    list of float values\n",
    "#   FUNCTION_CODING\n",
    "def forward_propagate(network, row):\n",
    "    inputs = row\n",
    "    for layer in network:\n",
    "        new_inputs = list()\n",
    "        for neuron in layer:\n",
    "            activation_value = activate_neuron(neuron['weights'], inputs)\n",
    "            neuron['output'] = transfer(activation_value)\n",
    "            new_inputs.append(neuron['output'])\n",
    "        inputs = new_inputs\n",
    "        \n",
    "    return inputs\n",
    "#   FUNCTION_EXPLANATION\n",
    "#   Propagates the values from a row of values\n",
    "#   till the output's layer and returns the last\n",
    "#   list of values\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Backward propagate\n",
    "\n",
    "Esta sección de trabajo es la que permite determinar errores comparando el valor esperado con el valor propagado de la red. Este error es revisado a través de la red desde la capa final hasta la capa escondida asignando responsabilidad sobre el error y actualizando los pesos a medida que avanza.\n",
    "\n",
    "La propagación del error tiene una base científica en el *Cálculo*, sin embargo como simplificación nos mantendremos al margen de su funcionamiento y nos concentraremos en su uso.\n",
    "\n",
    "Al igual que la sección anterior, podemos dividir el trabajo en dos partes\n",
    "\n",
    "1. Derivada de transferencia\n",
    "2. Error de propagación hacia atras\n",
    "-----------------\n",
    "Lo primero es calcular la derivada de transferencia que para este informe estamos utilizando la función **Sigmoid**, cuya derivada se expresa de la siguiente forma\n",
    "\n",
    "\\begin{equation*}\n",
    "S'(t) = S(t)(1 - S(t))\n",
    "\\end{equation*}\n",
    "\n",
    "Esto nos permite implementarla de una manera muy sencilla"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#            transfer_derivative\n",
    "#--------------------------------------------\n",
    "#   FUNCTION_IN_PARAMETERS_DEFINITION\n",
    "#   S:   float number representing S(t)\n",
    "#\n",
    "#   FUNCTION_OUT_PARAMETERS_DEFINITION\n",
    "#   out:    float number \n",
    "#   FUNCTION_CODING\n",
    "def transfer_derivative(S):\n",
    "    derivative = S * (1.0 - S)\n",
    "    \n",
    "    return derivative\n",
    "#   FUNCTION_EXPLANATION\n",
    "#   Calculate the transfer derivative \n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---------------\n",
    "Ahora que tenemos la función implementada, debemos calcular el error para cada salida de la neurona, esto nos dará la señal de error que propagaremos hacia atrás a través de la red.\n",
    "\n",
    "Para obtener el error que mencionamos recién es necesario realizar una comparación entre el valor esperado y el valor resultante, a lo cual además multiplicaremos la derivada de la transferencia, esto se expresa matemáticamente como\n",
    "\n",
    "\\begin{equation*}\n",
    "error = (E(x) - O(x)) * \\frac{\\partial Sigmoid(x)}{\\partial x}\n",
    "\\end{equation*}\n",
    "\n",
    "Este cálculo de error es usado para neuronas en la capa de salida, donde el valor esperado es el valor de la clase. La señal para una neurona en la capa escondida es calculada como el error de los pesos de cada neurona en la capa de salida.\n",
    "\n",
    "Ahora bien, la señal del error propagado se acumula y usa para determinar el error de la neurona en la capa escondida de la siguiente forma\n",
    "\n",
    "\\begin{equation*}\n",
    "Error_{propagado} = (weight_i * error_j) * \\partial {transfer} \n",
    "\\end{equation*}\n",
    "\n",
    "Podemos observar como el error_j es la señal de la j-ésima neurona en la capa de salida, el weight_k es el peso que conecta la k-ésima neurona con la neurona actual mientras que la derivada de la transferencia se calcula con el valor de salida de la neurona actual.\n",
    "\n",
    "Como sabemos que nuestra red es un tipo de dato ordenado, por lo que para ir desde la capa final hasta la inicial, basta con recorrerla de manera inversa, es así como debemos calcular el error de la neurona y almacenarlo en la misma, a este error le llamaremos delta para representar el cambio que este involucra. Se podrá apreciar que la señal de error para las neuronas de la capa oculta es un valor acumulativo que proviene desde la capa de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#--------------------------------------------\n",
    "#            backward_propagate\n",
    "#--------------------------------------------\n",
    "#   FUNCTION_IN_PARAMETERS_DEFINITION\n",
    "#   network:   list of lists\n",
    "#   expected:  list of floats\n",
    "#\n",
    "#   FUNCTION_OUT_PARAMETERS_DEFINITION\n",
    "#   out:    boolean of aplication where True\n",
    "#           means a correctfull run of the algorithm\n",
    "#   FUNCTION_CODING\n",
    "def backward_propagate(network, expected):\n",
    "    try:\n",
    "        size = len(network)\n",
    "        iteration = range(size)[::-1]\n",
    "        for i in iteration:\n",
    "            layer = network[i]\n",
    "            errors = list()\n",
    "            if i != (size - 1):\n",
    "                for j in range(len(layer)):\n",
    "                    error = 0.0\n",
    "                    for neuron in network[i + 1]:\n",
    "                        error += neuron['weights'][j] * neuron['delta']\n",
    "                    errors.append(error)\n",
    "            else:\n",
    "                for j in range(len(layer)):\n",
    "                    neuron = layer[j]\n",
    "                    errors.append(expected[j] - neuron['output'])\n",
    "            for k in range(len(layer)):\n",
    "                neuron = layer[k]\n",
    "                neuron['delta'] = errors[k] * transfer_derivative(neuron['output'])\n",
    "        return 0\n",
    "    except:\n",
    "        return 1\n",
    "#   FUNCTION_EXPLANATION\n",
    "#   Propagates de errors and blame the current neuron\n",
    "#   by adapting its values\n",
    "#--------------------------------------------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Entrenamiento de la red\n",
    "\n",
    "La red será entrenada utilizando el metodo SGD *(Stochastic Gradient Descent)*. Esto involucra exponer los datos de entrenamiento multiples veces a la red y que por cada fila de datos propagar las entradas hacia adelante y propagar los errores hacia atras para actualizar los pesos de la red.\n",
    "\n",
    "Este proceso, al igual como hemos realizado en todo el trabajo, puede descomponerse en dos partes\n",
    "\n",
    "1. Actualizar Pesos\n",
    "2. Entrenar la red\n",
    "---------------------\n",
    "Dado que ya hemos calculado los errores para cada neurona en nuestra red, podemos utilizar dicho error para actualizar los pesos. Si definimos al peso como **_W_**, un parametro de aprendizaje **_L<sub>r</sub>_**, el error como un delta **$\\delta$** y una entrada, causante del error como **_I_**.\n",
    "\n",
    "\\begin{equation*}\n",
    "W_{new} = W_{prev} + L_r *  \\delta * I\n",
    "\\end{equation*}\n",
    "\n",
    "De la misma forma, se puede actualizar el sesgo con la excepción de que no tiene un parametro de entrada o bien, puede ser evaluado como un parametro de entrada *1.0*.\n",
    "\n",
    "El paramentro de aprendizaje, referido como *Learning Rate*, controla cuanto cambiar el peso para corregir el error en términos porcentuales. Siempre es preferible utilizar valores de aprendizaje bajos por el impacto que este puede tener.\n",
    "\n",
    "Una implementación para la actualización de los pesos debe recorrer las neuronas y aplicar la ecuación descrita, cabe resaltar que las entradas utilizadas por capa son referentes a las salidas de la capa anterior."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def update_weights(network, row, lr):\n",
    "    for i in range(len(network)):\n",
    "        inputs = row[:-1]\n",
    "        if i != 0:\n",
    "            inputs = [neuron['output'] for neuron in network[i - 1]]\n",
    "        for neuron in network[i]:\n",
    "            for j in range(len(inputs)):\n",
    "                neuron['weights'][j] += lr * neuron['delta'] * inputs[j]\n",
    "            neuron['weights'][-1] += lr * neuron['delta']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "--------------------\n",
    "Una vez que ya tenemos nuestra función de actualización debemos generar el procedimiento para el entrenamiento, el cual se basa en una red inicializada con un *DataSet* entregado, un *learning_rate*, un número de *epoch* (época, o generación) y un número esperado de valores de salida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_network(network, train, lr, n_epoch, n_outputs):\n",
    "    errors = list()\n",
    "    for epoch in range(n_epoch):\n",
    "        sum_error = 0\n",
    "        for row in train:\n",
    "            outputs = forward_propagate(network, row)\n",
    "            expected = [0 for i in range(n_outputs)]\n",
    "            expected[row[-1] - 1] = 1\n",
    "            sum_error += sum([(expected[i] - outputs[i])**2 for i in range(len(expected))])\n",
    "            backward_propagate(network, expected)\n",
    "            update_weights(network, row, lr)\n",
    "        errors.append(sum_error)\n",
    "    return errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**B) Programa de predicción**\n",
    "\n",
    "Un programa que logre realizar una predicción suena algo descabellado, sin embargo, ya teniendo una red neuronal entrenada, podemos realizar una implementación sencilla para generar una solución. Como tenemos la función *forward_propagate*, podemos utilizar los valores de salida que nos genera como una predicción.\n",
    "\n",
    "En matemática existe una definición en particular que utilizaremos, el **arg max**, siendo los puntos del dominio de una funcion donde los valores son máximos. [Leer más detalles](https://en.wikipedia.org/wiki/Arg_max) Esto es para retornar el valor de salida en la posición de mayor probabilidad."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(network, row):\n",
    "    outputs = forward_propagate(network, row)\n",
    "    return outputs.index(max(outputs))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos una función para realizar predicciones en base a *forward_propagate* debemos vectorizarla y admitirle un conjunto de ejemplos mayor. Como sabemos que la función aplica sobre una fila de valores, un conjuntos de varios ejemplos estará definido como un dataset de varias filas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def multi_predict(network, examples):\n",
    "    predictions = list()\n",
    "    for row in examples:\n",
    "        prediction = predict(network, row)\n",
    "        predictions.append((row[-1], prediction))\n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**C) Comprobación de datos**\n",
    "\n",
    "Para demostrar el funcionamiento del algoritmo utilizaremos utilizaremos un dataset de clasificación de semillas el cual podemos encontrar en [la siguiente url](http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt). Los datos se normalizan según lo entregado en el enunciado de la tarea."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "url = 'http://archive.ics.uci.edu/ml/machine-learning-databases/00236/seeds_dataset.txt'\n",
    "df = pd.read_csv(url, sep=r'\\s+', header=None)\n",
    "X_train = df.ix[:,0:6]\n",
    "y_train = df.ix[:,7]\n",
    "scaler = StandardScaler().fit(X_train)\n",
    "X_train = scaler.transform(X_train)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora que tenemos normalizados los datos, debemos convertirlos a un formato legible por nuestra implementación, la cual debe tener los valores de las etiquetas **y_train** como final de las filas del dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "DataSeeds = X_train.tolist()\n",
    "for idx, row in enumerate(DataSeeds):\n",
    "    row.append(y_train[idx])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora tomamos los datos, inicializamos la red y generamos nuestra solución"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_inputs = 7\n",
    "n_outputs = 3\n",
    "n_hidden = 5 # Alterable\n",
    "\n",
    "rd.seed(1)\n",
    "network = initialize_network(n_inputs, n_hidden, n_outputs)\n",
    "\n",
    "n_epochs = 400\n",
    "learning_rate = 0.5\n",
    "\n",
    "errors = train_network(network, DataSeeds, learning_rate, n_epochs, n_outputs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
